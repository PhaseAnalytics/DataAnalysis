## Explore the data. Provide summary statistics and a visualization for the variable columns (one at a time, or in combination). In a brief paragraph, summarize the distributions for variable values and explain your choice of visualization. 

For summary statistics I generated the standard statistics of mean, standard deviation, max, min, and quartiles (e.g., the same as in Pandas describe). I did find that the dataset included two NaN values, which I replaced using the mean of the data from that variable. One immediate observation about the dataset is that all the values lie between -2 and 2. One visualization I prefer for datasets with many variables is the heatmap, which is a great way to visually detect patterns in the dataset based on vertical or horizontal striations. Some observations from the heatmap include: larger than normal values in sample sets around 100 as seen in the dark red horizontal striation, and many of the variables are shifted towards the negative values as seen by multiple blue vertical striations. 
![Dataset Visualization][DatasetViz]

### Present a Principal Components Analysis (PCA) plot for the samples. It should contain a scatterplot of the sample points with the axes PC1 vs. PC2. Indicate on the plot which samples have class = 1 and which have class = 0. 
![PCA Visualization][PCAViz]

### Calculate a statistic for every variable that describes its relationship with the class column. Don't list them all, but for the variable column with the most significant statistic, provide a visualization that shows its relationship with class. Include a brief paragraph describing your choices of statistic and visualization. 

Because the class column is boolean I selected the point biserial correlation coefficient, which is a modified version of Pearsonâ€™s correlation coefficient that adjusts for one variable consisting of only two states. The most significant variable was selected by choosing the variable with the smallest p-value when computing the point biserial coefficient. For a visualization I selected the two dimensional kernel density estimator. Although the KDE is inaccurate for binomial data, it shows a density around zero and one, the visualization is useful to see how the variables are distributed together in two space. 
![Point Biseral KDE][KDE1Viz]

### Calculate a statistic for every variable that describes its relationship with PC1 (i.e. the first principal component). For the variable with the most significant statistic, provide a visualization that shows its relationship with PC1. Include a brief paragraph describing your choices of statistic and visualization. 

With the first principal component being continuous as is the variable data I selected the standard Pearson correlation coefficient to describe the relationship between PC1 and the data set. The most significant variable was selected by choosing the variable with the smallest p-value when computing the Pearson correlation coefficient. For a visualization I again selected the two dimensional kernel density estimator. I find the KDE to be one of the better visualizations to observe how two variables relate with one another. 
![Pearson Correlation KDE][KDE2Viz]

### Create a classifier model predicting class of each sample using some or all of the variables in the dataset. Use cross-validation to calculate the effectiveness of your classifier. Provide a short paragraph detailing your rationale for picking a classifier method, selecting a subset of variables for the model (if you did this), followed by a summary of your classifier's performance. 

Because the dataset had many variables, few samples, and because there was not a good separation between classes in the variables I selected the random forest classifier. I often use the random forest classifier when the classes are not well separated as it is effective at finding localized areas within the entire space of the data where the classes are separable, and it does this without significant user tuning. To tune the model I ran a grid search on three random forest parameters: number of estimators, max depth of trees, and max number of features to use when choosing splits. Additionally, I ran this grid search for on subsets of the variable from the model. I selected subsets of variables by selecting the top N most statistically significant variables (based on the statistic from part 3a). In this stage the top N variables were selected from the range of: all, and 5-20. The best performing model parameters were determined to be: number of estimators=15, max depth = 2, max features = 50% of all features, and selecting the top 16 statistically significant variables. With these parameters the 10-fold cross validation was around 76%, which was better than the value of 72% achieved without tuning the model. Note, I choose not to normalize the dataset. I did this because the all of the data feel within -2 and 2, so the range was not significantly different for the variables. 

[DatasetViz]: https://github.com/PhaseAnalytics/DataAnalysis/blob/master/challenge_one/files/Data_Heatmap.png
[PCAViz]: https://github.com/PhaseAnalytics/DataAnalysis/blob/master/challenge_one/files/PCA_scatterplot.png
[KDE1Viz]: https://github.com/PhaseAnalytics/DataAnalysis/blob/master/challenge_one/files/KDE_SigVar_Class.png
[KDE2Viz]: https://github.com/PhaseAnalytics/DataAnalysis/blob/master/challenge_one/files/KDE_SigVar_FirstPCA.png

